<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>wwwflypaper -  Generate a real-looking bogus web page</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rev="made" href="mailto:root@localhost" />
</head>

<body style="background-color: white">

<p><a name="__index__"></a></p>
<!-- INDEX BEGIN -->

<ul>

	<li><a href="#name">NAME</a></li>
	<li><a href="#synopsis">SYNOPSIS</a></li>
	<li><a href="#options">OPTIONS</a></li>
	<li><a href="#arguments">ARGUMENTS</a></li>
	<li><a href="#description">DESCRIPTION</a></li>
	<li><a href="#examples">EXAMPLES</a></li>
	<li><a href="#troubleshooting">TROUBLESHOOTING</a></li>
	<li><a href="#environment">ENVIRONMENT</a></li>
	<li><a href="#files">FILES</a></li>
	<li><a href="#see_also">SEE ALSO</a></li>
	<li><a href="#standards">STANDARDS</a></li>
	<li><a href="#bugs">BUGS</a></li>
	<li><a href="#availability">AVAILABILITY</a></li>
	<li><a href="#script_categories">SCRIPT CATEGORIES</a></li>
	<li><a href="#prerequisites">PREREQUISITES</a></li>
	<li><a href="#corequisites">COREQUISITES</a></li>
	<li><a href="#osnames">OSNAMES</a></li>
	<li><a href="#author">AUTHOR</a></li>
</ul>
<!-- INDEX END -->

<hr />
<p>
</p>
<hr />
<h1><a name="name">NAME</a></h1>
<p>wwwflypaper -  Generate a real-looking bogus web page</p>
<p>
</p>
<hr />
<h1><a name="synopsis">SYNOPSIS</a></h1>
<pre>
  wwwflypaper(.pl) [OPTIONS] [URLROOT]</pre>
<p>
</p>
<hr />
<h1><a name="options">OPTIONS</a></h1>
<dl>
<dt><strong><a name="item__2d_2dhelp_7c_2d_2dhelp_2dhtml_7c_2d_2dhelp_2dman"><strong>--help|--help-html|--help-man</strong></a></strong>

<dd>
<p>Print help. The default format is text. Option <strong>--help-html</strong> displays
help in HTML format and --help-man in Unix manual page format.</p>
</dd>
</li>
<dt><strong><a name="item__2d_2dversion"><strong>--version</strong></a></strong>

<dd>
<p>Print version information.</p>
</dd>
</li>
</dl>
<p>
</p>
<hr />
<h1><a name="arguments">ARGUMENTS</a></h1>
<dl>
<dt><strong><a name="item_urlroot"><strong>URLROOT</strong></a></strong>

<dd>
<p>In the generated web pages there are links that point back to the
page. These links (A HREF's in HTML) are by default prefixed with
<code>/email</code>. If the trap in the web server is put somewhere else, the
change must be told to the program in order to generate correct links.
See exmaples for more information.</p>
</dd>
</li>
</dl>
<p>
</p>
<hr />
<h1><a name="description">DESCRIPTION</a></h1>
<p>Generate random list of real-looking bogus email addresses in the
form of a HTML mailing list page.</p>
<p>The page is dynamically made and full of convincingly lifelike - but
completely bogus - email addresses that spambots can pick up and add
to their hitlists. The page also contains randomly generated links
that the bot inevitably follows - links that loop right back to the
same page, now re-armed with a fresh set of random fake email
addresses.</p>
<p>This program can be used to combat the junk email problem by
effectively poisoning the databases of those gathering programs that
regularly scan web pages looking for email addresses to harvest. The
program is designed to be as fast as possible and self containing. No
separate dictionary files are needed.</p>
<p>The restrictive licensing terms of <code>wpoison(1)</code> inspired to
write a free alternative -- this program.</p>
<p>
</p>
<hr />
<h1><a name="examples">EXAMPLES</a></h1>
<p>Let's suppose that local web server's robots file is located at
<em>$WWWROOT/robots.txt</em> announces a forbidden fruit. The well behaving
search robots will ignore all URLs that are banned by the robots file.
But the email harvesting programs will surely want to take a look into
the URLs mentioned in one or more <strong>Disallow</strong> lines in the
<em>robots.txt</em> file:</p>
<pre>
    User-agent: *
    Disallow: /email</pre>
<p>The idea is that system administrator will set up a trap, where every
access to pages under disallowed URLs lead to calls to this program.
Here is an example setup for Apache web server.</p>
<pre>
    &lt;Location /email&gt;
        &lt;IfModule mod_rewrite.c&gt;
            RewriteEngine  On
            RewriteRule    ^  /cgi-bin/wwwflypaper.pl
        &lt;/IfModule&gt;</pre>
<pre>
        AddType cgi-script html
        options +ExecCGI -Indexes
    &lt;/Location&gt;</pre>
<p>Place the program under <em>$WWWCGIBIN</em> directory (e.g. <code>
/usr/lib/cgi-bin/</code> if that's the Web server's CGI root) and the fly
paper trap is ready. Notice that the <code>caret(^)</code> matches every page
referred under <code>/email</code> location and redirects it to the program.
Verify the effect by accessing the page with browser:</p>
<pre>
   <a href="http://your.example.com/email">http://your.example.com/email</a></pre>
<p>The default A HEREF links are set to <em>/email</em>, but this needs to
be changed from command line if another URL location in the web
server is used. Like if the trap were in:</p>
<pre>
    wwwflypaper.pl /mailing-list</pre>
<p>
</p>
<hr />
<h1><a name="troubleshooting">TROUBLESHOOTING</a></h1>
<p>None.</p>
<p>
</p>
<hr />
<h1><a name="environment">ENVIRONMENT</a></h1>
<p>None used.</p>
<p>
</p>
<hr />
<h1><a name="files">FILES</a></h1>
<p>None used.</p>
<p>
</p>
<hr />
<h1><a name="see_also">SEE ALSO</a></h1>
<p><code>wpoison(1)</code></p>
<p>The wpoison &lt;http://www.monkeys.com/wpoison&gt;
does not comply with Debian Free Software Guidelines. See
&lt;http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=122929&gt; and
discussion at
&lt;http://lists.debian.org/debian-legal/2001/12/msg00388.html&gt;
&lt;http://lists.debian.org/debian-legal/2001/12/msg00396.html&gt;
&lt;http://lists.debian.org/debian-legal/2001/12/msg00410.html&gt;</p>
<p>
</p>
<hr />
<h1><a name="standards">STANDARDS</a></h1>
<p>The output is HTML 4.01 Transitional valid. See
&lt;http://www.w3.org/TR/html401/sgml/loosedtd.html&gt;. Web servers and
robots.txt standard is at &lt;http://www.robotstxt.org&gt;.</p>
<p>
</p>
<hr />
<h1><a name="bugs">BUGS</a></h1>
<p>The programs is designed as fast as possible and to be completely self
standing. There are no plans to support external lookup dictionaries
or other external configurations.</p>
<p>
</p>
<hr />
<h1><a name="availability">AVAILABILITY</a></h1>
<p>&lt;URL Where to get the program&gt;</p>
<p>
</p>
<hr />
<h1><a name="script_categories">SCRIPT CATEGORIES</a></h1>
<p>CPAN/Administrative</p>
<p>
</p>
<hr />
<h1><a name="prerequisites">PREREQUISITES</a></h1>
<p>No Perl CPAN modules are used.</p>
<p>
</p>
<hr />
<h1><a name="corequisites">COREQUISITES</a></h1>
<p>Only modules from standard Perl are used.</p>
<p>
</p>
<hr />
<h1><a name="osnames">OSNAMES</a></h1>
<p><code>any</code></p>
<p>
</p>
<hr />
<h1><a name="author">AUTHOR</a></h1>
<p>Copyright (C) 2005-2012 Jari Aalto.
This program is free software; you can redistribute and/or modify program
under the same terms as Perl itself or in terms of Gnu General Public
licence v2 or later.</p>

</body>

</html>
